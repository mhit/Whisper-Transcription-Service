#!/usr/bin/env python3
"""
Ollamaãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ã®æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import yaml
import os
from pathlib import Path

def verify_ollama_logic():
    """OllamaãŒæ­£ã—ãèªè­˜ã•ã‚Œã‚‹ã‹æ¤œè¨¼"""

    print("=" * 60)
    print("ğŸ” Ollama Logic Verification")
    print("=" * 60)

    # 1. config.yamlã‚’èª­ã¿è¾¼ã‚€
    config_path = Path("config.yaml")
    if not config_path.exists():
        print("âŒ config.yaml not found")
        return False

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    analyzer_config = config.get('analyzer', {})
    ollama_config = analyzer_config.get('ollama_fallback', {})

    # 2. Ollamaè¨­å®šã‚’ãƒã‚§ãƒƒã‚¯
    print("\nğŸ“‹ Configuration Check:")
    print(f"  Ollama enabled: {ollama_config.get('enabled', False)}")
    print(f"  API base URL: {analyzer_config.get('api_base_url', 'Not set')}")
    print(f"  Model: {ollama_config.get('model', 'Not set')}")

    # 3. æ–°ã—ã„ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    openai_api_key = os.getenv('OPENAI_API_KEY')

    # æ—§ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒã‚°ã‚ã‚Šï¼‰
    old_logic_result = bool(openai_api_key)

    # æ–°ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰
    is_ollama_enabled = (
        ollama_config.get('enabled', False) or
        (analyzer_config.get('api_base_url') and 'localhost:11434' in analyzer_config.get('api_base_url', ''))
    )
    new_logic_result = bool(openai_api_key) or is_ollama_enabled

    # 4. çµæœã‚’è¡¨ç¤º
    print("\nğŸ”¬ Logic Analysis:")
    print(f"  OpenAI API Key: {'Set' if openai_api_key else 'Not set'}")
    print(f"  Ollama enabled: {is_ollama_enabled}")
    print(f"  Old logic (buggy): AI analysis would run = {old_logic_result}")
    print(f"  New logic (fixed): AI analysis would run = {new_logic_result}")

    # 5. å•é¡Œã®è¨ºæ–­
    print("\nğŸ“Š Diagnosis:")
    if not old_logic_result and is_ollama_enabled:
        print("  âœ… BUG FIXED: Previously AI analysis was skipped even with Ollama enabled")
        print("  âœ… Now AI analysis will run with Ollama!")
    elif old_logic_result == new_logic_result:
        print("  â„¹ï¸  No change in behavior (both conditions give same result)")
    else:
        print("  ğŸ” Edge case detected")

    # 6. ã‚³ãƒ¼ãƒ‰ã®ç¢ºèª
    print("\nğŸ“ Code Verification:")
    try:
        with open("video_transcript_analyzer.py", 'r', encoding='utf-8') as f:
            content = f.read()

        # ä¿®æ­£ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if "self.is_ollama_enabled" in content:
            print("  âœ… is_ollama_enabled flag found in code")
        else:
            print("  âŒ is_ollama_enabled flag NOT found - fix not applied?")

        if "elif self.openai_api_key or self.is_ollama_enabled:" in content:
            print("  âœ… Fixed condition found at line 250")
        else:
            print("  âŒ Fixed condition NOT found - using old buggy logic?")

    except Exception as e:
        print(f"  âŒ Could not verify code: {e}")

    print("\n" + "=" * 60)
    print("ğŸ¯ Summary:")
    if new_logic_result and is_ollama_enabled:
        print("  âœ… Ollama will be used for AI analysis!")
        print("  Run: python video_transcript_analyzer.py --input [video]")
    else:
        print("  âš ï¸  AI analysis will be skipped")
        print("  Check your Ollama configuration in config.yaml")
    print("=" * 60)

if __name__ == "__main__":
    verify_ollama_logic()