"""
ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
æˆåŠŸã—ã¦ã„ãŸ generate_seminar_report.py ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ¡ç”¨

ä¸»ãªç‰¹å¾´ï¼š
- æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ï¼ˆ10åˆ†å˜ä½ï¼‰
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®é‡è¦åº¦åˆ¤å®š
- é‡è¦ãƒã‚¤ãƒ³ãƒˆã®ç›´æ¥æŠ½å‡º
- ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹æœçš„ãªè¦ç´„ç”Ÿæˆ
"""

import logging
import time
import json
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from .keyword_analyzer import KeywordAnalyzer


@dataclass
class SimpleSummaryResult:
    """ã‚·ãƒ³ãƒ—ãƒ«è¦ç´„ã®çµæœã‚’æ ¼ç´"""
    segment_summaries: List[Dict[str, Any]]  # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¦ç´„ï¼ˆ10åˆ†ã”ã¨ï¼‰
    key_moments: List[Dict[str, Any]]        # é‡è¦ãªç¬é–“
    executive_summary: str                    # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼
    metadata: Dict[str, Any]                  # ãƒ¡ã‚¿æƒ…å ±


class SimpleSummarizer:
    """æˆåŠŸã—ãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ³ãƒ—ãƒ«è¦ç´„ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆæœŸåŒ–

        Args:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.logger = logging.getLogger('VideoTranscriptAnalyzer.simple_summarizer')

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè¨­å®š
        self.segment_minutes = config.get('segment_minutes', 10)  # 10åˆ†å˜ä½

        # å‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æå™¨ã‚’åˆæœŸåŒ–
        self.keyword_analyzer = KeywordAnalyzer()
        self.content_analysis = None  # åˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥

        # é™çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
        self.static_keywords = config.get('importance_keywords', [
            # ãƒ“ã‚¸ãƒã‚¹ãƒ»ãƒãƒã‚¿ã‚¤ã‚ºé–¢é€£
            'åç›Š', 'å£²ä¸Š', 'åˆ©ç›Š', 'ä¸‡å††', 'å„„å††', 'ãƒãƒã‚¿ã‚¤ã‚º', 'åç›ŠåŒ–', 'ãƒ“ã‚¸ãƒã‚¹',

            # æˆåŠŸãƒ»å®Ÿç¸¾é–¢é€£
            'æˆåŠŸ', 'é”æˆ', 'å®Ÿç¸¾', 'çµæœ', 'æˆæœ', 'åŠ¹æœ', 'æ”¹å–„', 'å‘ä¸Š',

            # æˆ¦ç•¥ãƒ»æ–¹æ³•é–¢é€£
            'æˆ¦ç•¥', 'æ–¹æ³•', 'ã‚³ãƒ„', 'ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯', 'ãƒã‚¤ãƒ³ãƒˆ', 'ç§˜è¨£', 'ãƒã‚¦ãƒã‚¦', 'æ‰‹æ³•',

            # é‡è¦æ€§ã‚’ç¤ºã™è¨€è‘‰
            'é‡è¦', 'å¤§åˆ‡', 'å¿…è¦', 'å¿…é ˆ', 'åŸºæœ¬', 'æœ¬è³ª', 'æ ¸å¿ƒ', 'ã‚­ãƒ¼',

            # å•é¡Œãƒ»èª²é¡Œé–¢é€£
            'å•é¡Œ', 'èª²é¡Œ', 'å¤±æ•—', 'ãƒŸã‚¹', 'æ³¨æ„', 'æ°—ã‚’ã¤ã‘', 'ãƒªã‚¹ã‚¯', 'ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ',

            # å…·ä½“çš„ãªæ•°å€¤ãƒ»ãƒ‡ãƒ¼ã‚¿
            'ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ', '%', 'å€', 'å¢—åŠ ', 'æ¸›å°‘', 'å¹³å‡', 'ãƒ‡ãƒ¼ã‚¿', 'çµ±è¨ˆ',

            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é–¢é€£
            'å®Ÿè·µ', 'å®Ÿè¡Œ', 'ã‚„ã‚Šæ–¹', 'ã‚¹ãƒ†ãƒƒãƒ—', 'æ‰‹é †', 'å°å…¥', 'æ´»ç”¨', 'ä½¿ã„æ–¹',

            # SNSãƒ»ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°å›ºæœ‰
            'ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', 'ã„ã„ã­', 'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ', 'ãƒªãƒ¼ãƒ', 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³',
            'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³', 'CTR', 'ROI', 'KPI', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ', 'ãƒšãƒ«ã‚½ãƒŠ'
        ])

        # LLMè¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼šã‚ˆã‚Šè‰¯ã„è¦ç´„ã®ãŸã‚ï¼‰
        self.use_llm = config.get('use_llm', True)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§LLMã‚’æœ‰åŠ¹åŒ–ï¼ˆè©³ç´°ãªè¦ç´„ç”Ÿæˆï¼‰
        if self.use_llm:
            self.model_name = config.get('model', 'gpt-oss:20b')
            # LangChainç”¨ã«URLã‚’èª¿æ•´ï¼ˆ/v1ã‚’é™¤å»ï¼‰
            api_url = config.get('api_base_url', 'http://localhost:11434')
            if api_url.endswith('/v1'):
                self.api_base_url = api_url[:-3]
            else:
                self.api_base_url = api_url
            self.temperature = config.get('temperature', 0.3)
            self.max_tokens = config.get('max_tokens', 500)

    def analyze(self,
                transcript_data: Dict[str, Any],
                output_dir: Path) -> SimpleSummaryResult:
        """
        ã‚·ãƒ³ãƒ—ãƒ«è¦ç´„ã‚’å®Ÿè¡Œ

        Args:
            transcript_data: æ–‡å­—èµ·ã“ã—ãƒ‡ãƒ¼ã‚¿
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            ã‚·ãƒ³ãƒ—ãƒ«è¦ç´„çµæœ
        """
        self.logger.info("ã‚·ãƒ³ãƒ—ãƒ«è¦ç´„åˆ†æã‚’é–‹å§‹...")
        start_time = time.time()

        try:
            # 0. å…¨ä½“ãƒ†ã‚­ã‚¹ãƒˆã®å‹•çš„åˆ†æ
            all_text = ' '.join([seg.get('text', '') for seg in transcript_data.get('segments', [])])
            self.logger.info("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã‚’å®Ÿè¡Œä¸­...")
            self.content_analysis = self.keyword_analyzer.analyze_content(all_text)
            self.logger.info(f"ãƒ‰ãƒ¡ã‚¤ãƒ³æ¨å®š: {self.content_analysis['domain']}")
            self.logger.info(f"é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(self.content_analysis['important_keywords'])}")

            # 1. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ï¼ˆ10åˆ†å˜ä½ï¼‰
            segments = self._segment_transcript(transcript_data)
            self.logger.info(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(segments)}")

            # 2. å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è¦ç´„ç”Ÿæˆ
            segment_summaries = []
            for i, segment in enumerate(segments):
                summary = self._summarize_segment(segment, i + 1)
                segment_summaries.append(summary)
                self.logger.debug(f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {i+1}/{len(segments)} å‡¦ç†å®Œäº†")

            # 3. é‡è¦ãªç¬é–“ã‚’ç‰¹å®š
            key_moments = self._identify_key_moments(segment_summaries)
            self.logger.info(f"é‡è¦ãªç¬é–“: {len(key_moments)}å€‹")

            # 4. ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            executive_summary = self._generate_executive_summary(
                segment_summaries, key_moments
            )

            # 5. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            metadata = {
                'total_duration': transcript_data.get('duration', 0),
                'total_segments': len(segments),
                'total_words': sum(len(seg['text'].split()) for seg in segments),
                'processing_time': time.time() - start_time,
                'segment_minutes': self.segment_minutes,
                'domain': self.content_analysis.get('domain', 'unknown') if self.content_analysis else 'unknown',
                'top_keywords': [kw['keyword'] for kw in self.content_analysis['important_keywords'][:10]] if self.content_analysis else [],
                'numeric_summary': {
                    k: len(v) for k, v in self.content_analysis['numeric_patterns'].items()
                } if self.content_analysis else {}
            }

            # çµæœã‚’è¿”ã™
            result = SimpleSummaryResult(
                segment_summaries=segment_summaries,
                key_moments=key_moments,
                executive_summary=executive_summary,
                metadata=metadata
            )

            # çµæœã‚’ä¿å­˜
            self._save_results(result, output_dir)

            elapsed = time.time() - start_time
            self.logger.info(f"âœ… ã‚·ãƒ³ãƒ—ãƒ«è¦ç´„å®Œäº†ï¼ˆ{elapsed:.1f}ç§’ï¼‰")

            return result

        except Exception as e:
            self.logger.error(f"è¦ç´„å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
            raise

    def _segment_transcript(self, transcript_data: Dict[str, Any]) -> List[Dict]:
        """æ–‡å­—èµ·ã“ã—ã‚’æ™‚é–“ãƒ™ãƒ¼ã‚¹ã§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–"""
        segments = []
        segment_seconds = self.segment_minutes * 60

        current_segment = {
            'start_time': 0,
            'end_time': segment_seconds,
            'text': '',
            'raw_segments': []
        }

        for seg in transcript_data.get('segments', []):
            seg_start = seg.get('start', 0)
            seg_text = seg.get('text', '').strip()

            # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ™‚é–“å†…ã®å ´åˆ
            if seg_start < current_segment['end_time']:
                current_segment['text'] += ' ' + seg_text
                current_segment['raw_segments'].append(seg)
            else:
                # æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ç§»è¡Œ
                if current_segment['text'].strip():
                    segments.append(current_segment)

                # æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–‹å§‹
                current_segment = {
                    'start_time': current_segment['end_time'],
                    'end_time': current_segment['end_time'] + segment_seconds,
                    'text': seg_text,
                    'raw_segments': [seg]
                }

        # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
        if current_segment['text'].strip():
            segments.append(current_segment)

        return segments

    def _summarize_segment(self, segment: Dict, segment_num: int) -> Dict[str, Any]:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è¦ç´„"""
        text = segment['text'].strip()

        # æ–‡ã‚’åˆ†å‰²
        sentences = self._split_sentences(text)

        # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        importance_score = self._calculate_importance_score(text)

        # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
        key_points = self._extract_key_points(sentences, max_points=5)

        # è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ
        if self.use_llm:
            summary_text = self._generate_llm_summary(text, key_points)
        else:
            # ã‚·ãƒ³ãƒ—ãƒ«ãªè¦ç´„ï¼ˆæœ€åˆã®500æ–‡å­—ã¾ãŸã¯é‡è¦ãªæ–‡ï¼‰
            if key_points:
                summary_text = ' '.join(key_points[:3])
            else:
                summary_text = text[:500] + '...' if len(text) > 500 else text

        return {
            'segment_number': segment_num,
            'start_time': segment['start_time'],
            'end_time': segment['end_time'],
            'text': summary_text,
            'key_points': key_points,
            'importance_score': importance_score,
            'word_count': len(text.split())
        }

    def _split_sentences(self, text: str) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ–‡ã«åˆ†å‰²"""
        # æ—¥æœ¬èªã®æ–‡æœ«è¨˜å·ã§åˆ†å‰²
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        return sentences

    def _calculate_importance_score(self, text: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆå‹•çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ï¼‰"""
        # å‹•çš„åˆ†æçµæœãŒã‚ã‚‹å ´åˆã¯ä½¿ç”¨
        if self.content_analysis:
            return self.keyword_analyzer.calculate_dynamic_importance_score(
                text, self.content_analysis
            )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šé™çš„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ç”¨
        score = 0
        text_lower = text.lower()

        for keyword in self.static_keywords:
            count = text_lower.count(keyword.lower())
            score += count

        # æ–‡å­—æ•°ã§æ­£è¦åŒ–ï¼ˆ1000æ–‡å­—ã‚ãŸã‚Šã®ã‚¹ã‚³ã‚¢ï¼‰
        if len(text) > 0:
            score = (score / len(text)) * 1000

        return round(score, 2)

    def _extract_key_points(self, sentences: List[str], max_points: int = 5) -> List[str]:
        """é‡è¦ãªæ–‡ã‚’æŠ½å‡º"""
        scored_sentences = []

        for sentence in sentences:
            score = self._calculate_importance_score(sentence)
            if score > 0:  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã®ã¿
                scored_sentences.append((sentence, score))

        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        # ä¸Šä½Nå€‹ã‚’é¸æŠï¼ˆãŸã ã—å…ƒã®é †åºã‚’ä¿æŒï¼‰
        selected = scored_sentences[:max_points]
        selected_sentences = [s[0] for s in selected]

        # å…ƒã®é †åºã§è¿”ã™
        key_points = []
        for sentence in sentences:
            if sentence in selected_sentences:
                key_points.append(sentence)
                if len(key_points) >= max_points:
                    break

        return key_points

    def _generate_llm_summary(self, text: str, key_points: List[str]) -> str:
        """LLMã‚’ä½¿ã£ãŸè©³ç´°ãªè¦ç´„ç”Ÿæˆ"""
        try:
            import requests
            import json

            # Ollama APIã‚’ç›´æ¥ä½¿ç”¨
            url = f"{self.api_base_url}/api/generate"

            # ã‚ˆã‚Šè©³ç´°ãªè¦ç´„ã‚’ç”Ÿæˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""ä»¥ä¸‹ã®ã‚»ãƒŸãƒŠãƒ¼å†…å®¹ã‚’åˆ†æã—ã€ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®ã‚ã‚‹è¦ç´„ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„ã®è¦ä»¶ã€‘
1. å…·ä½“çš„ãªæ•°å€¤ã‚„æˆæœãŒã‚ã‚Œã°å¿…ãšå«ã‚ã‚‹
2. å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚„ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’æŠ½å‡º
3. æˆåŠŸäº‹ä¾‹ã‚„å¤±æ•—äº‹ä¾‹ãŒã‚ã‚Œã°æ˜è¨˜
4. é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„æ¦‚å¿µã‚’å¼·èª¿

ã€ã‚»ãƒŸãƒŠãƒ¼å†…å®¹ã€‘
{text[:3000]}

ã€ç‰¹ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã€‘
{chr(10).join(f'â€¢ {point}' for point in key_points[:5])}

ã€è¦ç´„ã€‘ï¼ˆ300-500æ–‡å­—ã§æ§‹é€ åŒ–ã—ã¦è¨˜è¿°ï¼‰ï¼š"""

            # Ollama APIãƒªã‚¯ã‚¨ã‚¹ãƒˆ
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "temperature": self.temperature,
                "stream": False,
                "options": {
                    "num_predict": self.max_tokens
                }
            }

            response = requests.post(url, json=payload, timeout=30)

            if response.status_code == 200:
                result = response.json()
                summary = result.get('response', '').strip()
            else:
                raise Exception(f"APIè¿”å›é”™è¯¯: {response.status_code}")

            # è¦ç´„ãŒçŸ­ã™ãã‚‹å ´åˆã¯ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’è¿½åŠ 
            if len(summary) < 100 and key_points:
                summary += "\n\nã€æŠ½å‡ºã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒˆã€‘\n" + "\n".join(f"â€¢ {point}" for point in key_points[:3])

            return summary

        except Exception as e:
            self.logger.warning(f"LLMè¦ç´„å¤±æ•—ã€è©³ç´°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {e}")
            # ã‚ˆã‚Šè©³ç´°ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            fallback_parts = []

            if key_points:
                fallback_parts.append("ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘")
                for i, point in enumerate(key_points[:5], 1):
                    fallback_parts.append(f"{i}. {point}")

            # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤æƒ…å ±ã‚’æŠ½å‡º
            import re
            numbers = re.findall(r'\d+[ä¸‡å„„]?å††|\d+ä¸‡?ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼|\d+%', text)
            if numbers:
                fallback_parts.append("\nã€æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã€‘")
                fallback_parts.append("â€¢ " + ", ".join(set(numbers[:5])))

            # æˆåŠŸé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’æŠ½å‡º
            success_sentences = [s for s in self._split_sentences(text[:1000])
                                if any(kw in s for kw in ['æˆåŠŸ', 'é”æˆ', 'å®Ÿç¾', 'åç›Š'])]
            if success_sentences:
                fallback_parts.append("\nã€æˆæœãƒ»å®Ÿç¸¾ã€‘")
                fallback_parts.append(success_sentences[0][:200])

            return "\n".join(fallback_parts) if fallback_parts else text[:500] + '...'

    def _identify_key_moments(self, summaries: List[Dict], top_n: int = 10) -> List[Dict]:
        """é‡è¦ãªç¬é–“ã‚’ç‰¹å®š"""
        # é‡è¦åº¦ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        sorted_summaries = sorted(
            summaries,
            key=lambda x: x['importance_score'],
            reverse=True
        )

        # ä¸Šä½Nå€‹ã‚’é¸æŠ
        key_moments = []
        for summary in sorted_summaries[:top_n]:
            # å„é‡è¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ä¸­é–“æ™‚ç‚¹
            mid_time = (summary['start_time'] + summary['end_time']) / 2

            key_moments.append({
                'timestamp': mid_time,
                'segment_number': summary['segment_number'],
                'importance_score': summary['importance_score'],
                'description': summary['key_points'][0] if summary['key_points'] else summary['text'][:100],
                'start_time': summary['start_time'],
                'end_time': summary['end_time']
            })

        # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆ
        key_moments.sort(key=lambda x: x['timestamp'])

        return key_moments

    def _generate_executive_summary(self,
                                   summaries: List[Dict],
                                   key_moments: List[Dict]) -> str:
        """è©³ç´°ãªã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        import re

        # æœ€é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
        top_moments = sorted(
            key_moments,
            key=lambda x: x['importance_score'],
            reverse=True
        )[:5]

        # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        all_text = ' '.join([s['text'] for s in summaries])
        revenue_numbers = re.findall(r'\d+[ä¸‡å„„]?å††', all_text)
        follower_numbers = re.findall(r'\d+ä¸‡?ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼', all_text)
        percentages = re.findall(r'\d+[ï¼…%]', all_text)

        # ã‚µãƒãƒªãƒ¼æ§‹ç¯‰
        summary_parts = []

        # ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦
        summary_parts.append("# ğŸ“Š ã‚»ãƒŸãƒŠãƒ¼è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆ\n")
        summary_parts.append("## ğŸ¯ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼\n")

        # ã‚»ãƒŸãƒŠãƒ¼ã®ä¸»é¡Œã‚’ç‰¹å®šï¼ˆå‹•çš„åˆ†æçµæœã‹ã‚‰æ¨å®šï¼‰
        theme_keywords = []
        if self.content_analysis and 'important_keywords' in self.content_analysis:
            for kw_info in self.content_analysis['important_keywords'][:10]:
                kw = kw_info['keyword']
                if all_text.lower().count(kw.lower()) > 3:
                    theme_keywords.append(kw)

        if theme_keywords:
            summary_parts.append(f"**ä¸»è¦ãƒ†ãƒ¼ãƒ**: {', '.join(theme_keywords[:3])}\n\n")

        # æ ¸å¿ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        summary_parts.append("### ğŸ’¡ æ ¸å¿ƒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n")
        if top_moments:
            for i, moment in enumerate(top_moments[:3], 1):
                desc = moment['description']
                # é•·ã™ãã‚‹å ´åˆã¯è¦ç´„
                if len(desc) > 150:
                    desc = desc[:150] + "..."
                summary_parts.append(f"{i}. **{desc}**\n")

        # æ•°å€¤ã§è¦‹ã‚‹æˆæœ
        if revenue_numbers or follower_numbers:
            summary_parts.append("\n### ğŸ“ˆ æ•°å€¤ã§è¦‹ã‚‹æˆæœ\n")
            if revenue_numbers:
                unique_revenues = list(set(revenue_numbers))[:5]
                summary_parts.append(f"- **åç›Šå®Ÿç¸¾**: {', '.join(unique_revenues)}\n")
            if follower_numbers:
                unique_followers = list(set(follower_numbers))[:5]
                summary_parts.append(f"- **ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°**: {', '.join(unique_followers)}\n")
            if percentages:
                unique_percentages = list(set(percentages))[:3]
                summary_parts.append(f"- **æˆé•·ç‡**: {', '.join(unique_percentages)}\n")

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        summary_parts.append("\n### ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n")

        # é«˜ã‚¹ã‚³ã‚¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ™‚ç³»åˆ—é †ã«è¡¨ç¤º
        high_score_segments = [s for s in summaries if s['importance_score'] > 3][:5]
        high_score_segments.sort(key=lambda x: x['segment_number'])

        for seg in high_score_segments:
            time_range = f"{seg['start_time']//60:.0f}åˆ†-{seg['end_time']//60:.0f}åˆ†"
            score = seg['importance_score']

            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Œã°æœ€åˆã®1ã¤ã‚’ä½¿ç”¨
            if seg['key_points']:
                highlight = seg['key_points'][0][:100]
            else:
                highlight = seg['text'][:100]

            summary_parts.append(f"- **[{time_range}]** (é‡è¦åº¦: {score:.1f}) {highlight}...\n")

        # ãƒ¡ã‚¿æƒ…å ±
        summary_parts.append(f"\n### ğŸ“Š åˆ†ææ¦‚è¦\n")
        summary_parts.append(f"- **ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°**: {len(summaries)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆï¼ˆ{self.segment_minutes}åˆ†å˜ä½ï¼‰\n")
        summary_parts.append(f"- **ç·å˜èªæ•°**: {sum(s['word_count'] for s in summaries):,}èª\n")
        summary_parts.append(f"- **é‡è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³**: {len([s for s in summaries if s['importance_score'] > 5])}å€‹\n")

        return ''.join(summary_parts)

    def _save_results(self, result: SimpleSummaryResult, output_dir: Path):
        """çµæœã‚’ä¿å­˜"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # JSONå½¢å¼ã§ä¿å­˜
        result_dict = {
            'segment_summaries': result.segment_summaries,
            'key_moments': result.key_moments,
            'executive_summary': result.executive_summary,
            'metadata': result.metadata,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }

        output_file = output_dir / 'simple_summary.json'
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result_dict, f, ensure_ascii=False, indent=2)

        self.logger.info(f"è¦ç´„çµæœã‚’ä¿å­˜: {output_file}")