# Ollama用設定例
# Ollamaをローカルで起動した状態で使用
# 起動コマンド: ollama serve
# モデルのダウンロード: ollama pull llama2

# 作業ディレクトリの設定
work_dir: ./output

# AI分析設定（Ollama）
analyzer:
  # OllamaのAPIエンドポイント
  api_base_url: http://localhost:11434/v1

  # APIタイプ
  api_type: custom

  # モデル設定（Ollamaでダウンロードしたモデル名を指定）
  # 利用可能なモデル例:
  # - llama2:7b, llama2:13b, llama2:70b
  # - mistral:7b
  # - mixtral:8x7b
  # - codellama:7b
  # - phi-2
  model: llama2:7b
  temperature: 0.7
  max_tokens: 4000

  # 分析タイプ（有効/無効を選択）
  analysis_types:
    summary: true
    key_points: true
    topics: true
    sentiment: true
    keywords: true

  chunk_size: 8000  # Ollamaモデルに合わせて調整
  chunk_overlap: 500
  enable_basic_analysis: true

# その他の設定はデフォルトのconfig.yamlと同じ
transcriber:
  model: large-v3
  language: ja
  device: auto

reporter:
  format: markdown
  include_screenshots: true
  screenshot_count: 10