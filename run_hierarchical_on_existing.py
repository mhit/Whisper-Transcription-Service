#!/usr/bin/env python3
"""
æ—¢å­˜ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦éšå±¤çš„è¦ç´„ã‚’å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
seminar_transcript_synced.txt ã‚’èª­ã¿è¾¼ã¿ã€éšå±¤çš„è¦ç´„ã‚’é©ç”¨
"""

import sys
import re
import json
from pathlib import Path
from typing import List, Dict, Any
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

from modules.hierarchical_analyzer import HierarchicalAnalyzer
from modules.utils import setup_logging


def parse_existing_transcript(file_path: str) -> Dict[str, Any]:
    """
    æ—¢å­˜ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ã€éšå±¤çš„è¦ç´„ç”¨ã®å½¢å¼ã«å¤‰æ›

    Args:
        file_path: ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        éšå±¤çš„è¦ç´„ã«é©ã—ãŸå½¢å¼ã®è¾æ›¸
    """
    print(f"ğŸ“„ èª­ã¿è¾¼ã¿ä¸­: {file_path}")

    segments = []
    total_duration = 0

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³: [XXX.XXåˆ† - XXX.XXåˆ†] ãƒ†ã‚­ã‚¹ãƒˆ
    pattern = r'\[(\d+\.\d+)åˆ† - (\d+\.\d+)åˆ†\] (.+)'

    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    print(f"ğŸ“ {len(lines)}è¡Œã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # ãƒ¡ã‚¿æƒ…å ±ã‚’æ¢ã™
    for line in lines[:10]:
        if 'ç·æ™‚é–“:' in line:
            match = re.search(r'(\d+\.\d+)åˆ†', line)
            if match:
                total_duration = float(match.group(1)) * 60  # ç§’ã«å¤‰æ›
                print(f"â±ï¸ ç·æ™‚é–“: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†)")

    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
    for line in lines:
        match = re.match(pattern, line.strip())
        if match:
            start_min = float(match.group(1))
            end_min = float(match.group(2))
            text = match.group(3)

            segment = {
                'start': start_min * 60,  # ç§’ã«å¤‰æ›
                'end': end_min * 60,      # ç§’ã«å¤‰æ›
                'text': text,
                'confidence': 0.95,  # æ—¢å­˜ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãªã®ã§é«˜ä¿¡é ¼åº¦
                'avg_logprob': -0.1,
                'compression_ratio': 1.2,
                'no_speech_prob': 0.01
            }
            segments.append(segment)

    print(f"âœ… {len(segments)}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

    # éšå±¤çš„è¦ç´„ç”¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ä½œæˆ
    transcript_data = {
        'segments': segments,
        'text': '\n'.join([seg['text'] for seg in segments]),
        'language': 'ja',
        'duration': total_duration if total_duration > 0 else segments[-1]['end'] if segments else 0
    }

    return transcript_data


def run_hierarchical_analysis(transcript_data: Dict[str, Any], output_dir: Path):
    """
    éšå±¤çš„è¦ç´„ã‚’å®Ÿè¡Œ

    Args:
        transcript_data: ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ‡ãƒ¼ã‚¿
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    print("\n" + "="*60)
    print("ğŸ¯ éšå±¤çš„è¦ç´„å®Ÿè¡Œ (LangChain + LlamaIndex)")
    print("="*60)

    # è¨­å®šï¼ˆconfig.yamlã‹ã‚‰èª­ã¿è¾¼ã‚€ã‹ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
    config_file = Path('config.yaml')
    if config_file.exists():
        import yaml
        with open(config_file, 'r', encoding='utf-8') as f:
            full_config = yaml.safe_load(f)
            # hierarchical_summarizationè¨­å®šã‚’å–å¾—
            config = full_config.get('hierarchical_summarization', {})
            # analyzerè¨­å®šã‹ã‚‰api_base_urlã‚’å–å¾—
            analyzer_config = full_config.get('analyzer', {})
            if 'api_base_url' in analyzer_config:
                config['api_base_url'] = analyzer_config['api_base_url']
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        config = {
            'levels': 3,
            'segment_duration': 600,  # 10åˆ†
            'reduction_ratio': 0.4,
            'model': 'gpt-oss:20b',  # Ollamaãƒ¢ãƒ‡ãƒ«
            'temperature': 0.3,
            'max_tokens': 2000,
            'api_base_url': 'http://localhost:11434/v1',  # OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            'cache_dir': './cache',
            'parallel_processing': True,
            'max_workers': 4
        }

    # HierarchicalAnalyzerã®åˆæœŸåŒ–
    print("\nğŸš€ éšå±¤çš„è¦ç´„ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...")
    analyzer = HierarchicalAnalyzer(config)

    # éšå±¤çš„è¦ç´„ã®å®Ÿè¡Œ
    print("\nğŸ“Š éšå±¤çš„è¦ç´„å‡¦ç†ä¸­...")
    print("   ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™...")

    try:
        result = analyzer.analyze(transcript_data, output_dir)

        print("\n" + "="*60)
        print("ğŸ“ˆ çµæœ")
        print("="*60)

        # Level 1ã®çµæœ
        print(f"\nğŸ“ Level 1 (è©³ç´°è¦ç´„)")
        print(f"   ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(result.level1_summaries)}")
        if result.level1_summaries:
            first = result.level1_summaries[0]
            print(f"   ã‚µãƒ³ãƒ—ãƒ«: [{first['start_time']:.1f}ç§’-{first['end_time']:.1f}ç§’]")
            print(f"   å†…å®¹: {first['text'][:200]}...")

        # Level 2ã®çµæœ
        print(f"\nğŸ“ Level 2 (ä¸­é–“è¦ç´„)")
        print(f"   ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(result.level2_summaries)}")
        if result.level2_summaries:
            first = result.level2_summaries[0]
            print(f"   ã‚°ãƒ«ãƒ¼ãƒ— {first['group_id']+1}:")
            print(f"   å†…å®¹: {first['text'][:200]}...")

        # Level 3ã®çµæœ
        print(f"\nğŸ“ Level 3 (æœ€çµ‚çµ±åˆè¦ç´„)")
        if result.level3_summary:
            print("="*60)
            print(result.level3_summary['text'])
            print("="*60)

        # é‡è¦ãªç¬é–“
        print(f"\nğŸŒŸ é‡è¦ãªç¬é–“ ({len(result.key_moments)}å€‹)")
        for i, moment in enumerate(result.key_moments[:5], 1):
            print(f"\n{i}. [{moment['start_time']/60:.1f}åˆ†] (é‡è¦åº¦: {moment['importance_score']:.2f})")
            print(f"   ç†ç”±: {moment['reason']}")
            print(f"   å†…å®¹: {moment['preview'][:100]}...")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        print(f"\nğŸ“Š çµ±è¨ˆ")
        print(f"   å‡¦ç†æ™‚é–“: {result.metadata['processing_time']:.1f}ç§’")
        print(f"   ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {result.metadata['total_segments']}")
        print(f"   åœ§ç¸®é”æˆç‡: {result.metadata['reduction_achieved']:.1%}")

        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")

        # JSONã¨ã—ã¦ä¿å­˜
        output_file = output_dir / "hierarchical_summary_from_existing.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'level1': result.level1_summaries,
                'level2': result.level2_summaries,
                'level3': result.level3_summary,
                'key_moments': result.key_moments,
                'metadata': result.metadata
            }, f, ensure_ascii=False, indent=2)

        print(f"âœ… çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")

        # Markdownãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ä¿å­˜
        report_file = output_dir / "hierarchical_summary_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# éšå±¤çš„è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆ\n\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {result.metadata.get('timestamp', '')}\n\n")

            f.write("## ğŸ“ çµ±åˆè¦ç´„ (Level 3)\n\n")
            f.write(result.level3_summary['text'])
            f.write("\n\n")

            f.write("## ğŸ¯ ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¦ç´„ (Level 2)\n\n")
            for summary in result.level2_summaries:
                f.write(f"### ã‚°ãƒ«ãƒ¼ãƒ— {summary['group_id']+1}\n")
                f.write(f"**æ™‚é–“ç¯„å›²**: {summary['start_time']/60:.1f}åˆ† - {summary['end_time']/60:.1f}åˆ†\n\n")
                f.write(summary['text'])
                f.write("\n\n")

            f.write("## ğŸŒŸ é‡è¦ãªç¬é–“\n\n")
            for i, moment in enumerate(result.key_moments[:10], 1):
                f.write(f"{i}. **[{moment['start_time']/60:.1f}åˆ†]** (é‡è¦åº¦: {moment['importance_score']:.1%})\n")
                f.write(f"   - {moment['preview']}\n")
                f.write(f"   - ç†ç”±: {moment['reason']}\n\n")

            f.write("## ğŸ“Š çµ±è¨ˆ\n\n")
            f.write(f"- å‡¦ç†æ™‚é–“: {result.metadata['processing_time']:.1f}ç§’\n")
            f.write(f"- ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {result.metadata['total_segments']}\n")
            f.write(f"- åœ§ç¸®é”æˆç‡: {result.metadata['reduction_achieved']:.1%}\n")

        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_file}")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¨­å®š
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("="*60)
    print("ğŸ”¬ æ—¢å­˜ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§éšå±¤çš„è¦ç´„ãƒ†ã‚¹ãƒˆ")
    print("="*60)

    # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
    logger = setup_logging({'level': 'INFO'})

    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    transcript_file = Path(r"C:\Users\mhit\Downloads\ã‚»ãƒŸãƒŠãƒ¼æ–‡å­—èµ·ã“ã—\seminar_transcript_synced.txt")

    if not transcript_file.exists():
        print(f"âŒ ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {transcript_file}")
        return

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = Path("output_existing_transcript")
    output_dir.mkdir(exist_ok=True)

    # 1. æ—¢å­˜ã®ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒ‘ãƒ¼ã‚¹
    print("\nğŸ“„ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ä¸­...")
    transcript_data = parse_existing_transcript(str(transcript_file))

    print(f"   - ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {len(transcript_data['segments'])}")
    print(f"   - ç·æ™‚é–“: {transcript_data['duration']/60:.1f}åˆ†")
    print(f"   - è¨€èª: {transcript_data['language']}")

    # 2. éšå±¤çš„è¦ç´„ã‚’å®Ÿè¡Œ
    print("\nğŸ¯ ã‚¹ãƒ†ãƒƒãƒ—2: éšå±¤çš„è¦ç´„ã‚’å®Ÿè¡Œä¸­...")
    run_hierarchical_analysis(transcript_data, output_dir)

    print("\n" + "="*60)
    print("âœ… å‡¦ç†å®Œäº†ï¼")
    print("="*60)
    print(f"\nçµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™:")
    print(f"  ğŸ“ {output_dir.absolute()}")
    print(f"     - hierarchical_summary_from_existing.json")
    print(f"     - hierarchical_summary_report.md")


if __name__ == "__main__":
    main()